{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define BASE_FOLDER globally\n",
    "BASE_FOLDER = \"data\"\n",
    "\n",
    "def make_dir():\n",
    "    \"\"\"Creates required directories if they don't exist.\"\"\"\n",
    "    sub_dirs = [\"raw\", \"raw/train\", \"raw/test\"]  # Fixed missing comma\n",
    "    for sub in sub_dirs:\n",
    "        os.makedirs(os.path.join(BASE_FOLDER, sub), exist_ok=True)\n",
    "\n",
    "def load_data(url):\n",
    "    \"\"\"Loads data from the provided URL.\"\"\"\n",
    "    return pd.read_csv(url)\n",
    "\n",
    "def processing(df):\n",
    "    \"\"\"Filters, maps sentiment values, and shuffles data.\"\"\"\n",
    "    return (\n",
    "        df[df[\"sentiment\"].isin([\"happiness\", \"sadness\"])]\n",
    "        .drop(columns=[\"tweet_id\"], errors=\"ignore\")  # Avoid KeyError if column is missing\n",
    "        .assign(sentiment=lambda x: x[\"sentiment\"].map({\"happiness\": 1, \"sadness\": 0}))\n",
    "        .dropna()\n",
    "        .sample(frac=1, random_state=42)  # Shuffle the data\n",
    "    )\n",
    "\n",
    "def save_data(final_df):\n",
    "    \"\"\"Splits data into train and test sets and saves them.\"\"\"\n",
    "    train_df, test_df = train_test_split(final_df, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_df.to_csv(os.path.join(BASE_FOLDER, \"raw/train\", \"train.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(BASE_FOLDER, \"raw/test\", \"test.csv\"), index=False)\n",
    "\n",
    "def main():\n",
    "    make_dir()\n",
    "    url = \"https://raw.githubusercontent.com/campusx-official/jupyter-masterclass/main/tweet_emotions.csv\"\n",
    "\n",
    "    df = load_data(url)\n",
    "    final_df = processing(df)\n",
    "    save_data(final_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/santoshkumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/santoshkumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/santoshkumar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# NLTK for natural language processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords    # For stopwords\n",
    "from nltk.tokenize import word_tokenize # For tokenization\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer # For stemming and lemmatization\n",
    "\n",
    "# Downloading NLTK data\n",
    "nltk.download('stopwords')   # Downloading stopwords data\n",
    "nltk.download('punkt')       # Downloading tokenizer data\n",
    "nltk.download('wordnet')     # Downloading WordNet data for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(text_series):\n",
    "    \"\"\"Cleans the text data by removing URLs, emails, numbers, and punctuation.\"\"\"\n",
    "    number_pattern = r\"(?<=\\D)\\d+|\\d+(?=\\D)\"  # Removes numbers but keeps letters\n",
    "    url_pattern = r\"https?://\\S+|www\\.\\S+\"\n",
    "    email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n",
    "    punctuation_pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "    return (\n",
    "        text_series.astype(str)  # Ensure text is string\n",
    "        .str.lower()\n",
    "        .str.replace(url_pattern, \" \", regex=True)\n",
    "        .str.replace(email_pattern, \" \", regex=True)\n",
    "        .str.replace(number_pattern, \" \", regex=True)\n",
    "        .str.replace(punctuation_pattern, \" \", regex=True)\n",
    "        .str.strip()\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)  # Normalize spaces\n",
    "    )\n",
    "\n",
    "def remove_short_words(text_series, min_length=3):\n",
    "    \"\"\"Removes words shorter than `min_length` characters.\"\"\"\n",
    "    return text_series.apply(lambda x: \" \".join([word for word in x.split() if len(word) >= min_length]))\n",
    "\n",
    "def lemmatization(text_series):\n",
    "    \"\"\"Lemmatizes words using WordNetLemmatizer.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return text_series.apply(lambda x: \" \".join([lemmatizer.lemmatize(word, pos=\"v\") for word in x.split()]))\n",
    "\n",
    "def remove_stopwords(text_series):\n",
    "    \"\"\"Removes stopwords from text.\"\"\"\n",
    "    stop_words = frozenset(stopwords.words(\"english\"))  # Faster lookup\n",
    "    return text_series.apply(lambda x: \" \".join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "def normalize(df):\n",
    "    \"\"\"Applies text preprocessing steps.\"\"\"\n",
    "    df[\"content\"] = data_cleaning(df[\"content\"])\n",
    "    df[\"content\"] = remove_short_words(df[\"content\"])\n",
    "    df[\"content\"] = lemmatization(df[\"content\"])\n",
    "    df[\"content\"] = remove_stopwords(df[\"content\"])\n",
    "    df[\"content\"]\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    train_data = pd.read_csv(\"/Users/santoshkumar/Data_science/PyTorch_guide/data/raw/train/train.csv\")\n",
    "    test_data = pd.read_csv(\"/Users/santoshkumar/Data_science/PyTorch_guide/data/raw/test/test.csv\")\n",
    "\n",
    "    # Transform the data\n",
    "    train_processed_data = normalize(train_data)\n",
    "    test_processed_data = normalize(test_data)\n",
    "\n",
    "    # Store the data inside data/processed\n",
    "    data_path = os.path.join(\"./data\", \"interim\")\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "    train_processed_data.to_csv(os.path.join(data_path, \"train_processed.csv\"), index=False)\n",
    "    test_processed_data.to_csv(os.path.join(data_path, \"test_processed.csv\"), index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv(\"/Users/santoshkumar/Data_science/PyTorch_guide/data/interim/train_processed.csv\")\n",
    "y=pd.read_csv(\"/Users/santoshkumar/Data_science/PyTorch_guide/data/interim/test_processed.csv\")\n",
    "x=x.dropna()\n",
    "y= y.dropna()\n",
    "\n",
    "x_train= x['content']\n",
    "x_test=y['content']\n",
    "y_train= x[\"sentiment\"]\n",
    "y_test=y[\"sentiment\"]\n",
    "\n",
    "x_train = x_train\n",
    "x_test= x_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words=\"english\", ngram_range=(1,2), min_df=5, max_df=0.8)\n",
    "\n",
    "# Fit and transform on training data\n",
    "x_train_tfidf = vectorizer.fit_transform(x_train)\n",
    "\n",
    "# Transform the test data\n",
    "x_test_tfidf = vectorizer.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>adorable</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>year old</th>\n",
       "      <th>years</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8293</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8294</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8295</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8296</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.403969</td>\n",
       "      <td>0.380663</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8297</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8298 rows 칑 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      able  absolutely  accept  access  account  act  actually  add  adorable  \\\n",
       "0      0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "1      0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "2      0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "3      0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "4      0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "...    ...         ...     ...     ...      ...  ...       ...  ...       ...   \n",
       "8293   0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "8294   0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "8295   0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "8296   0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "8297   0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "\n",
       "      afternoon  ...  year  year old  years  yep  yes  yesterday  youtube  \\\n",
       "0           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "1           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "2           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "3           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "4           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "...         ...  ...   ...       ...    ...  ...  ...        ...      ...   \n",
       "8293        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "8294        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "8295        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "8296        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "8297        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "\n",
       "           yum     yummy  yup  \n",
       "0     0.000000  0.000000  0.0  \n",
       "1     0.000000  0.000000  0.0  \n",
       "2     0.000000  0.000000  0.0  \n",
       "3     0.000000  0.000000  0.0  \n",
       "4     0.000000  0.000000  0.0  \n",
       "...        ...       ...  ...  \n",
       "8293  0.000000  0.000000  0.0  \n",
       "8294  0.000000  0.000000  0.0  \n",
       "8295  0.000000  0.000000  0.0  \n",
       "8296  0.403969  0.380663  0.0  \n",
       "8297  0.000000  0.000000  0.0  \n",
       "\n",
       "[8298 rows x 1000 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(x_train_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>adorable</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>year old</th>\n",
       "      <th>years</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.728505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2074 rows 칑 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      able  absolutely  accept  access  account  act  actually  add  adorable  \\\n",
       "0      0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "1      0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "2      0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "3      0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "4      0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "...    ...         ...     ...     ...      ...  ...       ...  ...       ...   \n",
       "2069   0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "2070   0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "2071   0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "2072   0.0         0.0     0.0     0.0      0.0  0.0  0.728505  0.0       0.0   \n",
       "2073   0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "\n",
       "      afternoon  ...  year  year old  years  yep  yes  yesterday  youtube  \\\n",
       "0           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "1           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "2           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "3           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "4           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "...         ...  ...   ...       ...    ...  ...  ...        ...      ...   \n",
       "2069        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "2070        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "2071        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "2072        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "2073        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "\n",
       "      yum  yummy  yup  \n",
       "0     0.0    0.0  0.0  \n",
       "1     0.0    0.0  0.0  \n",
       "2     0.0    0.0  0.0  \n",
       "3     0.0    0.0  0.0  \n",
       "4     0.0    0.0  0.0  \n",
       "...   ...    ...  ...  \n",
       "2069  0.0    0.0  0.0  \n",
       "2070  0.0    0.0  0.0  \n",
       "2071  0.0    0.0  0.0  \n",
       "2072  0.0    0.0  0.0  \n",
       "2073  0.0    0.0  0.0  \n",
       "\n",
       "[2074 rows x 1000 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_test_df = pd.DataFrame(x_test_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse TF-IDF matrix to dense NumPy array before converting to tensors\n",
    "X_train_tensor = torch.tensor(x_train_tfidf.toarray(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(x_test_tfidf.toarray(), dtype=torch.float32)\n",
    "\n",
    "# Convert labels to tensors\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8298, 1000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LogisticRegression                       [32]                      --\n",
       "較럭Linear: 1-1                            [32, 1]                   1,001\n",
       "==========================================================================================\n",
       "Total params: 1,001\n",
       "Trainable params: 1,001\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.03\n",
       "==========================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.13\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x).squeeze(1)  \n",
    "\n",
    "# Model Parameters\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "output_dim = 1  \n",
    "\n",
    "# Instantiate Model\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "\n",
    "# Print Model Summary (use batch_size=32 for better display)\n",
    "summary(model, input_size=(32, input_dim))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function (Cross Entropy includes Softmax)\n",
    "criterion = nn.BCEWithLogitsLoss()  # More stable than BCELoss\n",
    "\n",
    "# Define Optimizer (SGD)\n",
    "learning_rate = 0.005\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100  Loss: 213.0949\n",
      "Epoch: 200  Loss: 212.6650\n",
      "Epoch: 300  Loss: 212.2767\n",
      "Epoch: 400  Loss: 212.0544\n",
      "Epoch: 500  Loss: 211.9871\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500  # Number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print progress every 500 epochs\n",
    "    if (epoch + 1) % 100 == 0:  \n",
    "        print(f'Epoch: {epoch+1}  Loss: {total_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Model Evaluation:\n",
      "  - Accuracy: 0.7739\n",
      "  - Precision: 0.7863\n",
      "  - Recall: 0.7616\n",
      "  - F1 Score: 0.7738\n",
      "  - ROC-AUC Score: 0.8607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"Evaluates the model and prints key metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_y_true, all_y_pred, all_y_scores = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            y_batch = y_batch.float().view(-1, 1)  # Ensure correct shape\n",
    "            \n",
    "            y_logits = model(X_batch)\n",
    "            y_probs = torch.sigmoid(y_logits)\n",
    "            y_pred_labels = (y_probs >= 0.5).float()\n",
    "\n",
    "            all_y_true.extend(y_batch.cpu().numpy())  \n",
    "            all_y_pred.extend(y_pred_labels.cpu().numpy())  \n",
    "            all_y_scores.extend(y_probs.cpu().numpy())  \n",
    "\n",
    "    # Compute Metrics\n",
    "    accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "    precision = precision_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    recall = recall_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    f1 = f1_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    roc_auc = roc_auc_score(all_y_true, all_y_scores)\n",
    "\n",
    "    print(f\"  - Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  - Precision: {precision:.4f}\")\n",
    "    print(f\"  - Recall: {recall:.4f}\")\n",
    "    print(f\"  - F1 Score: {f1:.4f}\")\n",
    "    print(f\"  - ROC-AUC Score: {roc_auc:.4f}\\n\")\n",
    "\n",
    "# Run Final Evaluation\n",
    "print(\"\\nFinal Model Evaluation:\")\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "游댳 Sklearn Logistic Regression Results:\n",
      "Accuracy: 0.7739\n",
      "Precision: 0.7835\n",
      "Recall: 0.7664\n",
      "F1 Score: 0.7748\n",
      "ROC-AUC Score: 0.8647\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Train Logistic Regression Model\n",
    "sklearn_model = LogisticRegression(max_iter=1000)\n",
    "sklearn_model.fit(x_train_tfidf, y_train)  # Train on TF-IDF features\n",
    "\n",
    "# Predict on Test Data\n",
    "y_pred = sklearn_model.predict(x_test_tfidf)\n",
    "y_probs = sklearn_model.predict_proba(x_test_tfidf)[:, 1]  # Probabilities for ROC-AUC\n",
    "\n",
    "# Calculate Evaluation Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "# Print Metrics\n",
    "print(\"游댳 Sklearn Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ANN                                      [32, 1]                   --\n",
       "較럭Linear: 1-1                            [32, 32]                  32,032\n",
       "較럭ReLU: 1-2                              [32, 32]                  --\n",
       "較럭Dropout: 1-3                           [32, 32]                  --\n",
       "較럭Linear: 1-4                            [32, 1]                   33\n",
       "==========================================================================================\n",
       "Total params: 32,065\n",
       "Trainable params: 32,065\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 1.03\n",
       "==========================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.13\n",
       "Estimated Total Size (MB): 0.26\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout= nn.Dropout(.2)\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x= self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc2(x))  # Apply Sigmoid for binary classification\n",
    "        return x\n",
    "\n",
    "# Model Parameters\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = ANN(input_dim)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.BCELoss()  # Use BCEWithLogitsLoss() if no sigmoid in forward()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "\n",
    "# Print Model Summary\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(32, input_dim))  # batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 | Loss: 358.2644\n",
      "Epoch 20/200 | Loss: 353.7908\n",
      "Epoch 30/200 | Loss: 341.8072\n",
      "Epoch 40/200 | Loss: 317.9062\n",
      "Epoch 50/200 | Loss: 290.0093\n",
      "Epoch 60/200 | Loss: 266.1908\n",
      "Epoch 70/200 | Loss: 250.8556\n",
      "Epoch 80/200 | Loss: 239.6591\n",
      "Epoch 90/200 | Loss: 229.5809\n",
      "Epoch 100/200 | Loss: 224.1549\n",
      "Epoch 110/200 | Loss: 219.4491\n",
      "Epoch 120/200 | Loss: 215.3842\n",
      "Epoch 130/200 | Loss: 211.7769\n",
      "Epoch 140/200 | Loss: 210.2859\n",
      "Epoch 150/200 | Loss: 206.1332\n",
      "Epoch 160/200 | Loss: 205.0118\n",
      "Epoch 170/200 | Loss: 201.7789\n",
      "Epoch 180/200 | Loss: 199.9354\n",
      "Epoch 190/200 | Loss: 199.0067\n",
      "Epoch 200/200 | Loss: 196.9233\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "num_epochs = 200  # You can increase this\n",
    "verbose_step = 10  # Print loss every 10 epochs\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X_batch).squeeze()  # Squeeze to ensure correct shape\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print progress every `verbose_step` epochs\n",
    "    if (epoch + 1) % verbose_step == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation:\n",
      "Accuracy: 0.7748\n",
      "Precision: 0.7867\n",
      "Recall: 0.7635\n",
      "F1 Score: 0.7749\n",
      "ROC-AUC Score: 0.8628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"Evaluates the model and prints performance metrics.\"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    all_y_true, all_y_pred, all_y_scores = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            y_logits = model(X_batch).squeeze()\n",
    "            y_probs = y_logits.cpu().numpy()  # Convert to NumPy\n",
    "            y_pred_labels = (y_probs >= 0.5).astype(int)  # Apply threshold\n",
    "\n",
    "            all_y_true.extend(y_batch.cpu().numpy())\n",
    "            all_y_pred.extend(y_pred_labels)\n",
    "            all_y_scores.extend(y_probs)\n",
    "\n",
    "    # Compute Metrics\n",
    "    accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "    precision = precision_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    recall = recall_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    f1 = f1_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    roc_auc = roc_auc_score(all_y_true, all_y_scores)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Evaluate on Test Data\n",
    "print(\"Final Evaluation:\")\n",
    "evaluate_model(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
