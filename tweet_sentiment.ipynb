{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define BASE_FOLDER globally\n",
    "BASE_FOLDER = \"data\"\n",
    "\n",
    "def make_dir():\n",
    "    \"\"\"Creates required directories if they don't exist.\"\"\"\n",
    "    sub_dirs = [\"raw\", \"raw/train\", \"raw/test\"]  # Fixed missing comma\n",
    "    for sub in sub_dirs:\n",
    "        os.makedirs(os.path.join(BASE_FOLDER, sub), exist_ok=True)\n",
    "\n",
    "def load_data(url):\n",
    "    \"\"\"Loads data from the provided URL.\"\"\"\n",
    "    return pd.read_csv(url)\n",
    "\n",
    "def processing(df):\n",
    "    \"\"\"Filters, maps sentiment values, and shuffles data.\"\"\"\n",
    "    return (\n",
    "        df[df[\"sentiment\"].isin([\"happiness\", \"sadness\"])]\n",
    "        .drop(columns=[\"tweet_id\"], errors=\"ignore\")  # Avoid KeyError if column is missing\n",
    "        .assign(sentiment=lambda x: x[\"sentiment\"].map({\"happiness\": 1, \"sadness\": 0}))\n",
    "        .dropna()\n",
    "        .sample(frac=1, random_state=42)  # Shuffle the data\n",
    "    )\n",
    "\n",
    "def save_data(final_df):\n",
    "    \"\"\"Splits data into train and test sets and saves them.\"\"\"\n",
    "    train_df, test_df = train_test_split(final_df, test_size=0.2, random_state=42)\n",
    "\n",
    "    train_df.to_csv(os.path.join(BASE_FOLDER, \"raw/train\", \"train.csv\"), index=False)\n",
    "    test_df.to_csv(os.path.join(BASE_FOLDER, \"raw/test\", \"test.csv\"), index=False)\n",
    "\n",
    "def main():\n",
    "    make_dir()\n",
    "    url = \"https://raw.githubusercontent.com/campusx-official/jupyter-masterclass/main/tweet_emotions.csv\"\n",
    "\n",
    "    df = load_data(url)\n",
    "    final_df = processing(df)\n",
    "    save_data(final_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/santoshkumar/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/santoshkumar/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/santoshkumar/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "# NLTK for natural language processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords    # For stopwords\n",
    "from nltk.tokenize import word_tokenize # For tokenization\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer # For stemming and lemmatization\n",
    "\n",
    "# Downloading NLTK data\n",
    "nltk.download('stopwords')   # Downloading stopwords data\n",
    "nltk.download('punkt')       # Downloading tokenizer data\n",
    "nltk.download('wordnet')     # Downloading WordNet data for lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(text_series):\n",
    "    \"\"\"Cleans the text data by removing URLs, emails, numbers, and punctuation.\"\"\"\n",
    "    number_pattern = r\"(?<=\\D)\\d+|\\d+(?=\\D)\"  # Removes numbers but keeps letters\n",
    "    url_pattern = r\"https?://\\S+|www\\.\\S+\"\n",
    "    email_pattern = r\"\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b\"\n",
    "    punctuation_pattern = r\"[^\\w\\s]\"\n",
    "\n",
    "    return (\n",
    "        text_series.astype(str)  # Ensure text is string\n",
    "        .str.lower()\n",
    "        .str.replace(url_pattern, \" \", regex=True)\n",
    "        .str.replace(email_pattern, \" \", regex=True)\n",
    "        .str.replace(number_pattern, \" \", regex=True)\n",
    "        .str.replace(punctuation_pattern, \" \", regex=True)\n",
    "        .str.strip()\n",
    "        .str.replace(r\"\\s+\", \" \", regex=True)  # Normalize spaces\n",
    "    )\n",
    "\n",
    "def remove_short_words(text_series, min_length=3):\n",
    "    \"\"\"Removes words shorter than `min_length` characters.\"\"\"\n",
    "    return text_series.apply(lambda x: \" \".join([word for word in x.split() if len(word) >= min_length]))\n",
    "\n",
    "def lemmatization(text_series):\n",
    "    \"\"\"Lemmatizes words using WordNetLemmatizer.\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return text_series.apply(lambda x: \" \".join([lemmatizer.lemmatize(word, pos=\"v\") for word in x.split()]))\n",
    "\n",
    "def remove_stopwords(text_series):\n",
    "    \"\"\"Removes stopwords from text.\"\"\"\n",
    "    stop_words = frozenset(stopwords.words(\"english\"))  # Faster lookup\n",
    "    return text_series.apply(lambda x: \" \".join([word for word in x.split() if word not in stop_words]))\n",
    "\n",
    "def normalize(df):\n",
    "    \"\"\"Applies text preprocessing steps.\"\"\"\n",
    "    df[\"content\"] = data_cleaning(df[\"content\"])\n",
    "    df[\"content\"] = remove_short_words(df[\"content\"])\n",
    "    df[\"content\"] = lemmatization(df[\"content\"])\n",
    "    df[\"content\"] = remove_stopwords(df[\"content\"])\n",
    "    df[\"content\"]\n",
    "    return df\n",
    "\n",
    "def main():\n",
    "    train_data = pd.read_csv(\"/Users/santoshkumar/Data_science/PyTorch_guide/data/raw/train/train.csv\")\n",
    "    test_data = pd.read_csv(\"/Users/santoshkumar/Data_science/PyTorch_guide/data/raw/test/test.csv\")\n",
    "\n",
    "    # Transform the data\n",
    "    train_processed_data = normalize(train_data)\n",
    "    test_processed_data = normalize(test_data)\n",
    "\n",
    "    # Store the data inside data/processed\n",
    "    data_path = os.path.join(\"./data\", \"interim\")\n",
    "    os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "    train_processed_data.to_csv(os.path.join(data_path, \"train_processed.csv\"), index=False)\n",
    "    test_processed_data.to_csv(os.path.join(data_path, \"test_processed.csv\"), index=False)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.read_csv(\"/Users/santoshkumar/Data_science/PyTorch_guide/data/interim/train_processed.csv\")\n",
    "y=pd.read_csv(\"/Users/santoshkumar/Data_science/PyTorch_guide/data/interim/test_processed.csv\")\n",
    "x=x.dropna()\n",
    "y= y.dropna()\n",
    "\n",
    "x_train= x['content']\n",
    "x_test=y['content']\n",
    "y_train= x[\"sentiment\"]\n",
    "y_test=y[\"sentiment\"]\n",
    "\n",
    "x_train = x_train\n",
    "x_test= x_test.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words=\"english\", ngram_range=(1,2), min_df=5, max_df=0.8)\n",
    "\n",
    "# Fit and transform on training data\n",
    "x_train_tfidf = vectorizer.fit_transform(x_train)\n",
    "\n",
    "# Transform the test data\n",
    "x_test_tfidf = vectorizer.transform(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>adorable</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>year old</th>\n",
       "      <th>years</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8293</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8294</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8295</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8296</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.403969</td>\n",
       "      <td>0.380663</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8297</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8298 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      able  absolutely  accept  access  account  act  actually  add  adorable  \\\n",
       "0      0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "1      0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "2      0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "3      0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "4      0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "...    ...         ...     ...     ...      ...  ...       ...  ...       ...   \n",
       "8293   0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "8294   0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "8295   0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "8296   0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "8297   0.0         0.0     0.0     0.0      0.0  0.0       0.0  0.0       0.0   \n",
       "\n",
       "      afternoon  ...  year  year old  years  yep  yes  yesterday  youtube  \\\n",
       "0           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "1           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "2           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "3           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "4           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "...         ...  ...   ...       ...    ...  ...  ...        ...      ...   \n",
       "8293        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "8294        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "8295        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "8296        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "8297        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "\n",
       "           yum     yummy  yup  \n",
       "0     0.000000  0.000000  0.0  \n",
       "1     0.000000  0.000000  0.0  \n",
       "2     0.000000  0.000000  0.0  \n",
       "3     0.000000  0.000000  0.0  \n",
       "4     0.000000  0.000000  0.0  \n",
       "...        ...       ...  ...  \n",
       "8293  0.000000  0.000000  0.0  \n",
       "8294  0.000000  0.000000  0.0  \n",
       "8295  0.000000  0.000000  0.0  \n",
       "8296  0.403969  0.380663  0.0  \n",
       "8297  0.000000  0.000000  0.0  \n",
       "\n",
       "[8298 rows x 1000 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(x_train_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accept</th>\n",
       "      <th>access</th>\n",
       "      <th>account</th>\n",
       "      <th>act</th>\n",
       "      <th>actually</th>\n",
       "      <th>add</th>\n",
       "      <th>adorable</th>\n",
       "      <th>afternoon</th>\n",
       "      <th>...</th>\n",
       "      <th>year</th>\n",
       "      <th>year old</th>\n",
       "      <th>years</th>\n",
       "      <th>yep</th>\n",
       "      <th>yes</th>\n",
       "      <th>yesterday</th>\n",
       "      <th>youtube</th>\n",
       "      <th>yum</th>\n",
       "      <th>yummy</th>\n",
       "      <th>yup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2069</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2070</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2071</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2072</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.728505</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2073</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2074 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      able  absolutely  accept  access  account  act  actually  add  adorable  \\\n",
       "0      0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "1      0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "2      0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "3      0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "4      0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "...    ...         ...     ...     ...      ...  ...       ...  ...       ...   \n",
       "2069   0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "2070   0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "2071   0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "2072   0.0         0.0     0.0     0.0      0.0  0.0  0.728505  0.0       0.0   \n",
       "2073   0.0         0.0     0.0     0.0      0.0  0.0  0.000000  0.0       0.0   \n",
       "\n",
       "      afternoon  ...  year  year old  years  yep  yes  yesterday  youtube  \\\n",
       "0           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "1           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "2           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "3           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "4           0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "...         ...  ...   ...       ...    ...  ...  ...        ...      ...   \n",
       "2069        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "2070        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "2071        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "2072        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "2073        0.0  ...   0.0       0.0    0.0  0.0  0.0        0.0      0.0   \n",
       "\n",
       "      yum  yummy  yup  \n",
       "0     0.0    0.0  0.0  \n",
       "1     0.0    0.0  0.0  \n",
       "2     0.0    0.0  0.0  \n",
       "3     0.0    0.0  0.0  \n",
       "4     0.0    0.0  0.0  \n",
       "...   ...    ...  ...  \n",
       "2069  0.0    0.0  0.0  \n",
       "2070  0.0    0.0  0.0  \n",
       "2071  0.0    0.0  0.0  \n",
       "2072  0.0    0.0  0.0  \n",
       "2073  0.0    0.0  0.0  \n",
       "\n",
       "[2074 rows x 1000 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_test_df = pd.DataFrame(x_test_tfidf.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "tfidf_test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert sparse TF-IDF matrix to dense NumPy array before converting to tensors\n",
    "X_train_tensor = torch.tensor(x_train_tfidf.toarray(), dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(x_test_tfidf.toarray(), dtype=torch.float32)\n",
    "\n",
    "# Convert labels to tensors\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8298, 1000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "LogisticRegression                       [32]                      --\n",
       "├─Linear: 1-1                            [32, 1]                   1,001\n",
       "==========================================================================================\n",
       "Total params: 1,001\n",
       "Trainable params: 1,001\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.03\n",
       "==========================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 0.00\n",
       "Params size (MB): 0.00\n",
       "Estimated Total Size (MB): 0.13\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "\n",
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x).squeeze(1)  \n",
    "\n",
    "# Model Parameters\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "output_dim = 1  \n",
    "\n",
    "# Instantiate Model\n",
    "model = LogisticRegression(input_dim, output_dim)\n",
    "\n",
    "# Print Model Summary (use batch_size=32 for better display)\n",
    "summary(model, input_size=(32, input_dim))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Loss Function (Cross Entropy includes Softmax)\n",
    "criterion = nn.BCEWithLogitsLoss()  # More stable than BCELoss\n",
    "\n",
    "# Define Optimizer (SGD)\n",
    "learning_rate = 0.005\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 100  Loss: 213.0949\n",
      "Epoch: 200  Loss: 212.6650\n",
      "Epoch: 300  Loss: 212.2767\n",
      "Epoch: 400  Loss: 212.0544\n",
      "Epoch: 500  Loss: 211.9871\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 500  # Number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Forward pass\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print progress every 500 epochs\n",
    "    if (epoch + 1) % 100 == 0:  \n",
    "        print(f'Epoch: {epoch+1}  Loss: {total_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Model Evaluation:\n",
      "  - Accuracy: 0.7739\n",
      "  - Precision: 0.7863\n",
      "  - Recall: 0.7616\n",
      "  - F1 Score: 0.7738\n",
      "  - ROC-AUC Score: 0.8607\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluation Function\n",
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"Evaluates the model and prints key metrics.\"\"\"\n",
    "    model.eval()\n",
    "    all_y_true, all_y_pred, all_y_scores = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            y_batch = y_batch.float().view(-1, 1)  # Ensure correct shape\n",
    "            \n",
    "            y_logits = model(X_batch)\n",
    "            y_probs = torch.sigmoid(y_logits)\n",
    "            y_pred_labels = (y_probs >= 0.5).float()\n",
    "\n",
    "            all_y_true.extend(y_batch.cpu().numpy())  \n",
    "            all_y_pred.extend(y_pred_labels.cpu().numpy())  \n",
    "            all_y_scores.extend(y_probs.cpu().numpy())  \n",
    "\n",
    "    # Compute Metrics\n",
    "    accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "    precision = precision_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    recall = recall_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    f1 = f1_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    roc_auc = roc_auc_score(all_y_true, all_y_scores)\n",
    "\n",
    "    print(f\"  - Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  - Precision: {precision:.4f}\")\n",
    "    print(f\"  - Recall: {recall:.4f}\")\n",
    "    print(f\"  - F1 Score: {f1:.4f}\")\n",
    "    print(f\"  - ROC-AUC Score: {roc_auc:.4f}\\n\")\n",
    "\n",
    "# Run Final Evaluation\n",
    "print(\"\\nFinal Model Evaluation:\")\n",
    "evaluate_model(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔹 Sklearn Logistic Regression Results:\n",
      "Accuracy: 0.7739\n",
      "Precision: 0.7835\n",
      "Recall: 0.7664\n",
      "F1 Score: 0.7748\n",
      "ROC-AUC Score: 0.8647\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Train Logistic Regression Model\n",
    "sklearn_model = LogisticRegression(max_iter=1000)\n",
    "sklearn_model.fit(x_train_tfidf, y_train)  # Train on TF-IDF features\n",
    "\n",
    "# Predict on Test Data\n",
    "y_pred = sklearn_model.predict(x_test_tfidf)\n",
    "y_probs = sklearn_model.predict_proba(x_test_tfidf)[:, 1]  # Probabilities for ROC-AUC\n",
    "\n",
    "# Calculate Evaluation Metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "roc_auc = roc_auc_score(y_test, y_probs)\n",
    "\n",
    "# Print Metrics\n",
    "print(\"🔹 Sklearn Logistic Regression Results:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ANN                                      [32, 1]                   --\n",
       "├─Linear: 1-1                            [32, 32]                  32,032\n",
       "├─ReLU: 1-2                              [32, 32]                  --\n",
       "├─Dropout: 1-3                           [32, 32]                  --\n",
       "├─Linear: 1-4                            [32, 1]                   33\n",
       "==========================================================================================\n",
       "Total params: 32,065\n",
       "Trainable params: 32,065\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 1.03\n",
       "==========================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 0.01\n",
       "Params size (MB): 0.13\n",
       "Estimated Total Size (MB): 0.26\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class ANN(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 32)\n",
    "        self.fc2 = nn.Linear(32, 1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout= nn.Dropout(.2)\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x= self.dropout(x)\n",
    "        x = torch.sigmoid(self.fc2(x))  # Apply Sigmoid for binary classification\n",
    "        return x\n",
    "\n",
    "# Model Parameters\n",
    "input_dim = X_train_tensor.shape[1]\n",
    "model = ANN(input_dim)\n",
    "\n",
    "# Loss Function and Optimizer\n",
    "criterion = nn.BCELoss()  # Use BCEWithLogitsLoss() if no sigmoid in forward()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.005, weight_decay=1e-4)\n",
    "\n",
    "# Print Model Summary\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(32, input_dim))  # batch_size = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 | Loss: 358.2644\n",
      "Epoch 20/200 | Loss: 353.7908\n",
      "Epoch 30/200 | Loss: 341.8072\n",
      "Epoch 40/200 | Loss: 317.9062\n",
      "Epoch 50/200 | Loss: 290.0093\n",
      "Epoch 60/200 | Loss: 266.1908\n",
      "Epoch 70/200 | Loss: 250.8556\n",
      "Epoch 80/200 | Loss: 239.6591\n",
      "Epoch 90/200 | Loss: 229.5809\n",
      "Epoch 100/200 | Loss: 224.1549\n",
      "Epoch 110/200 | Loss: 219.4491\n",
      "Epoch 120/200 | Loss: 215.3842\n",
      "Epoch 130/200 | Loss: 211.7769\n",
      "Epoch 140/200 | Loss: 210.2859\n",
      "Epoch 150/200 | Loss: 206.1332\n",
      "Epoch 160/200 | Loss: 205.0118\n",
      "Epoch 170/200 | Loss: 201.7789\n",
      "Epoch 180/200 | Loss: 199.9354\n",
      "Epoch 190/200 | Loss: 199.0067\n",
      "Epoch 200/200 | Loss: 196.9233\n"
     ]
    }
   ],
   "source": [
    "# Training Parameters\n",
    "num_epochs = 200  # You can increase this\n",
    "verbose_step = 10  # Print loss every 10 epochs\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0.0\n",
    "    model.train()  # Set model to training mode\n",
    "\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        y_pred = model(X_batch).squeeze()  # Squeeze to ensure correct shape\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print progress every `verbose_step` epochs\n",
    "    if (epoch + 1) % verbose_step == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation:\n",
      "Accuracy: 0.7748\n",
      "Precision: 0.7867\n",
      "Recall: 0.7635\n",
      "F1 Score: 0.7749\n",
      "ROC-AUC Score: 0.8628\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def evaluate_model(model, data_loader):\n",
    "    \"\"\"Evaluates the model and prints performance metrics.\"\"\"\n",
    "    model.eval()  # Set to evaluation mode\n",
    "    all_y_true, all_y_pred, all_y_scores = [], [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            y_logits = model(X_batch).squeeze()\n",
    "            y_probs = y_logits.cpu().numpy()  # Convert to NumPy\n",
    "            y_pred_labels = (y_probs >= 0.5).astype(int)  # Apply threshold\n",
    "\n",
    "            all_y_true.extend(y_batch.cpu().numpy())\n",
    "            all_y_pred.extend(y_pred_labels)\n",
    "            all_y_scores.extend(y_probs)\n",
    "\n",
    "    # Compute Metrics\n",
    "    accuracy = accuracy_score(all_y_true, all_y_pred)\n",
    "    precision = precision_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    recall = recall_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    f1 = f1_score(all_y_true, all_y_pred, zero_division=0)\n",
    "    roc_auc = roc_auc_score(all_y_true, all_y_scores)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Evaluate on Test Data\n",
    "print(\"Final Evaluation:\")\n",
    "evaluate_model(model, test_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
